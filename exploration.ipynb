{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2fde5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import typing\n",
    "from collections import defaultdict, namedtuple\n",
    "from typing import Type\n",
    "\n",
    "import einops\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "from devinterp.optim.sgld import SGLD\n",
    "from devinterp.slt.sampler import estimate_learning_coeff_with_summary\n",
    "from devinterp.utils import evaluate_ce\n",
    "from devinterp.vis_utils import EpsilonBetaAnalyzer\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import wandb\n",
    "from pizza_clock.config import get_device\n",
    "from pizza_clock.dataset import AdditionDataset\n",
    "from pizza_clock.metrics import compute_gradient_similarity\n",
    "from pizza_clock.training import ModularAdditionModelTrainer\n",
    "from pathlib import Path\n",
    "from pizza_clock.dataset import get_train_val_data\n",
    "from pizza_clock.config import Config\n",
    "from functools import partial\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45191917",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_last_position(criterion, model, data):\n",
    "    x, y = data\n",
    "    out = model(x)\n",
    "    logits = out[:, -1, :]  # Get the last position's logits: [batch, vocab]\n",
    "    return criterion(logits, y), {\"output\": logits}\n",
    "\n",
    "evaluate_last_position_ce = partial(evaluate_last_position, F.cross_entropy)\n",
    "\n",
    "\n",
    "def estimate_llc_given_model(\n",
    "    model: t.nn.Module,\n",
    "    loader: t.utils.data.DataLoader,\n",
    "    evaluate: typing.Callable,\n",
    "    epsilon: float,\n",
    "    beta: float,\n",
    "    sampling_method: Type[t.optim.Optimizer] = SGLD,\n",
    "    localization: float = 5.0,\n",
    "    num_chains: int = 2,\n",
    "    num_draws: int = 500,\n",
    "    num_burnin_steps: int = 0,\n",
    "    num_steps_bw_draws: int = 1,\n",
    "    device: t.device = get_device(),\n",
    "    online: bool = True,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    # Copied from devinterp grokking notebook https://github.com/timaeus-research/devinterp/blob/main/examples/grokking.ipynb\n",
    "    sweep_stats = estimate_learning_coeff_with_summary(\n",
    "        model,\n",
    "        loader=loader,\n",
    "        evaluate=evaluate,\n",
    "        sampling_method=sampling_method,\n",
    "        optimizer_kwargs=dict(lr=epsilon, localization=localization, nbeta=beta),\n",
    "        num_chains=num_chains,  # How many independent chains to run\n",
    "        num_draws=num_draws,  # How many samples to draw per chain\n",
    "        num_burnin_steps=num_burnin_steps,  # How many samples to discard at the beginning of each chain\n",
    "        num_steps_bw_draws=num_steps_bw_draws,  # How many steps to take between each sample\n",
    "        device=device,\n",
    "        online=online,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    sweep_stats[\"llc/trace\"] = np.array(sweep_stats[\"llc/trace\"])\n",
    "    return sweep_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c971ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_config(dir_path: str):\n",
    "    config_json = json.load(open(f\"{dir_path}/config.json\", \"r\"))\n",
    "    config = Config(**config_json)\n",
    "    final_model = t.load(f\"{dir_path}/final_model.pt\", map_location=get_device(), weights_only=False)\n",
    "    all_models = [\n",
    "        t.load(f\"{dir_path}/model_{i}.pt\", map_location=get_device(), weights_only=False)\n",
    "        for i in range(len(list(Path(dir_path).glob(\"model_*.pt\"))))\n",
    "    ]\n",
    "    return final_model, config, all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4496acd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hanna/Library/Caches/pypoetry/virtualenvs/pizza-clock--vnaSbzg-py3.13/lib/python3.13/site-packages/devinterp/vis_utils.py:99: UserWarning: Epsilon values greater than 1e-2 typically lead to instability in the sampling process. Consider reducing epsilon to between 1e-6 and 1e-2.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]/Users/hanna/Library/Caches/pypoetry/virtualenvs/pizza-clock--vnaSbzg-py3.13/lib/python3.13/site-packages/devinterp/slt/sampler.py:117: UserWarning: Using passed in nbeta. Make sure callbacks are also initialized with the same nbeta.\n",
      "  warnings.warn(\n",
      "/Users/hanna/Library/Caches/pypoetry/virtualenvs/pizza-clock--vnaSbzg-py3.13/lib/python3.13/site-packages/devinterp/backends/default/slt/sampler.py:240: UserWarning: You are taking more draws than burn-in steps, your LLC estimates will likely be underestimates. Please check LLC chain convergence.\n",
      "  warnings.warn(\n",
      "/Users/hanna/Library/Caches/pypoetry/virtualenvs/pizza-clock--vnaSbzg-py3.13/lib/python3.13/site-packages/devinterp/backends/default/slt/sampler.py:244: UserWarning: You are taking more sample batches than there are dataloader batches available, this removes some randomness from sampling but is probably fine. (All sample batches beyond the number dataloader batches are cycled from the start, f.e. 9 samples from [A, B, C] would be [B, A, C, B, A, C, B, A, C].)\n",
      "  warnings.warn(\n",
      "/Users/hanna/Library/Caches/pypoetry/virtualenvs/pizza-clock--vnaSbzg-py3.13/lib/python3.13/site-packages/devinterp/backends/default/slt/sampler.py:285: UserWarning: If you're setting a nbeta or temperature in optimizer_kwargs, please also make sure to set it in the callbacks.\n",
      "  warnings.warn(\n",
      "/Users/hanna/Library/Caches/pypoetry/virtualenvs/pizza-clock--vnaSbzg-py3.13/lib/python3.13/site-packages/devinterp/slt/sampler.py:117: UserWarning: Using passed in nbeta. Make sure callbacks are also initialized with the same nbeta.\n",
      "  warnings.warn(\n",
      "/Users/hanna/Library/Caches/pypoetry/virtualenvs/pizza-clock--vnaSbzg-py3.13/lib/python3.13/site-packages/devinterp/backends/default/slt/sampler.py:240: UserWarning: You are taking more draws than burn-in steps, your LLC estimates will likely be underestimates. Please check LLC chain convergence.\n",
      "  warnings.warn(\n",
      "/Users/hanna/Library/Caches/pypoetry/virtualenvs/pizza-clock--vnaSbzg-py3.13/lib/python3.13/site-packages/devinterp/backends/default/slt/sampler.py:244: UserWarning: You are taking more sample batches than there are dataloader batches available, this removes some randomness from sampling but is probably fine. (All sample batches beyond the number dataloader batches are cycled from the start, f.e. 9 samples from [A, B, C] would be [B, A, C, B, A, C, B, A, C].)\n",
      "  warnings.warn(\n",
      "/Users/hanna/Library/Caches/pypoetry/virtualenvs/pizza-clock--vnaSbzg-py3.13/lib/python3.13/site-packages/devinterp/backends/default/slt/sampler.py:285: UserWarning: If you're setting a nbeta or temperature in optimizer_kwargs, please also make sure to set it in the callbacks.\n",
      "  warnings.warn(\n",
      "/Users/hanna/Library/Caches/pypoetry/virtualenvs/pizza-clock--vnaSbzg-py3.13/lib/python3.13/site-packages/devinterp/backends/default/slt/sampler.py:52: UserWarning: You are taking more sample batches than there are dataloader batches available, this removes some randomness from sampling but is probably fine. (All sample batches beyond the number dataloader batches are cycled from the start, f.e. 9 samples from [A, B, C] would be [B, A, C, B, A, C, B, A, C].)\n",
      "  warnings.warn(\n",
      "/Users/hanna/Library/Caches/pypoetry/virtualenvs/pizza-clock--vnaSbzg-py3.13/lib/python3.13/site-packages/devinterp/backends/default/slt/sampler.py:52: UserWarning: You are taking more sample batches than there are dataloader batches available, this removes some randomness from sampling but is probably fine. (All sample batches beyond the number dataloader batches are cycled from the start, f.e. 9 samples from [A, B, C] would be [B, A, C, B, A, C, B, A, C].)\n",
      "  warnings.warn(\n",
      "  4%|â–         | 1/25 [00:11<04:43, 11.81s/it]"
     ]
    }
   ],
   "source": [
    "dir_path = \"saved_models/2026-01-27/attn0.0_seed4\"\n",
    "final_model, config, all_models = load_model_and_config(dir_path)\n",
    "train_loader, _ = get_train_val_data(config, squeeze_targets=True)\n",
    "\n",
    "analyzer = EpsilonBetaAnalyzer()\n",
    "analyzer.configure_sweep(\n",
    "    llc_estimator=estimate_llc_given_model,\n",
    "    llc_estimator_kwargs=dict(\n",
    "        model=final_model,\n",
    "        evaluate=evaluate_last_position_ce,  # Use custom evaluate function\n",
    "        device=get_device(),\n",
    "        loader=train_loader,\n",
    "    ),\n",
    "    min_epsilon=3e-5,\n",
    "    max_epsilon=3e-1,\n",
    "    epsilon_samples=5,\n",
    "    min_beta=None,\n",
    "    max_beta=None,\n",
    "    beta_samples=5,\n",
    "    dataloader=train_loader,\n",
    ")\n",
    "analyzer.sweep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c049090c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pizza-clock--vnaSbzg-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
